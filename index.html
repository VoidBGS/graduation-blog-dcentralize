<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mutterings of a Mad Student</title>
    <link rel="stylesheet" href="css/styles.css">
    <script src="scripts/fontColor.js" defer></script> 
    <script src="scripts/smoothScroll.js" defer></script> 
    <link rel="icon" type="image/x-icon" href="images/favicon.ico">
</head>

<body>
    <h1 class="text-center">Blog</h1>
    <div class="marquee"><p>01001001 01101110 01110100 01100101 01110010 01100101 01110011 01110100 01101001 01101110 01100111 00101110 00101110 00101110</p></div>
    <div id="fast-scroll">
        <p id="UP">UP</p>
    </div>
    <span>
        <a href="#blog" id="blog_scroll">> Jump to the blog</a>
        <a href="#blog" id="blog_deep">> Jump in the deep end</a>

        <button id="fn-button">Fun Button</button>
    </span>
    <div id="about">
        <h2>About</h2>
        <p> Hey, here I will describe what I do each day of my internship at d-centralize, the company I managed to get into for the last year of uni.</p>
        <p> A blog is a good way to document my daily struggles as a new developer in a company that is fairly old now. I am lucky enough to have a handbook for almost all processes that occur at d-centralize, which will make my life easier in the beginning.</p>
        <h3>About > d-centralize</h3>
        <p> D-centralize is a company that operates from Strijp-S, Eindhoven. They build SaaS platforms that are made for an international audience. They work with Dutch and international students and focus on software quality and testing. They offer remote internships and challenging projects.</p>
        <p> The founder of the company is Kees van den Broek and the stakeholders are the clients of the company. The company focuses on providing high quality software for their clients. </p>
        <p> A lot of the programmers in the company are former or current Fontys students, so the workflow that they follow is reminiscent of how a very advanced Fontys university project would be built. They follow all steps of the software development process that I am familiar with like automatic testing, continuous integration and deployment, branching strategies and the use of the Agile methodology. Moreover, the software solutions that are designed by the company are decentralized meaning that they are spread into smaller systems that communicate with each other via API.</p>
        <h3>About > My Assignment</h3>
        <p> My assignment would be to implement new features to an already existing software that were planned by the team. The software itself is a website that aims to help annotators, speech, and language therapists by automatically determining which words are pronounced incorrectly. It utilizes AI for word recognition, and it displays the results using the abovementioned website. The AI in the software makes use of the expertise provided by speech therapists to learn which words are pronounced incorrectly.</p>
        <p> I will work on rethinking and restructuring the architectural decisions. Since the application works with humans there is also the aspect of false positives and negatives, so I need to work with the team to minimize those outcomes by creating a hypothesis, testing it, and evaluating the results. Since d-centralize already has an A.I model that it is trying to improve by restructuring the architectural decisions we can also say that one of the goals of this internship would be to improve the accuracy of that model. The technology stack that will be used in this assignment will be Python for the backend server and ReactJS (Typescript) for the client application. I may also make use of Gitlabs CI/CD pipeline feature for testing and automation.</p>
        <p> I believe that this project would be challenging, but suitable for me as it involves the whole process from development to deployment. It includes developing the full stack application with both Python and React (Typescript), the creation of multiple automated tests and the creation of a CI/CD pipeline that automates testing and deployment. I will gain experience in all those fields, and it will help me improve as a programmer. I believe that I can manage to complete this project and I will gain a lot from it as it is challenging, because of the multiple technologies and techniques involved. Another reason why I think this graduation would be suitable for me is that I will also gain experience by working with other, more experienced programmers and communicating with them daily.</p>
    </div>
    <div id="blog">
        <h2>Blog</h2>
        <h3>06.02.2023 | My first day in the company</h3>
        <p>My first day in the company went smooth. I received my company email, I used the email to log into all my newly created accounts and I gained access to the GitLab repository of the project that I will work on. I made some tasks for the day and I explored the annotator tool. I started by fixing some typos in the Readme.md file. This very simple step enabled me to learn the process of how the branching process is done in the company and how different issues are handled. After fixing some of the typos I went ahead to run the annotator tool locally.</p>
        <h3>07.02.2023 | First issue encountered</h3>
        <p>Today was marked with some issues related to one of the pages that exist inside the annotator tool frontend, which is built in ReactJS, using a combination of TypeScript and JavaScript. I was told that the initial project was created in JavaScript and afterwards they decided to move to TypeScript slowly, but I guess this process is not completed yet. Nevertheless I found an issue with one of the pages that caused the server to be flooded. The issue came when all organizations were loaded into the page, and instead of fetching the recording counts from the server, it instead just submits a request for each organization, in order to fetch the recordings. This is bad. It is a very inefficient way of fetching the counts, because let's say we have 2000 organizations and we have to fetch the counts for all of them. This means that we have to submit 2000 requests, if we are to distribute this system the server can get flooded simply by people visiting this page. I communicated this with my mentor and we decided that it is better to have pagination for the page and see if that fixes some of the slowness caused by that many requests to the server.</p>
        <h3>08.02.2023 | Pagination implementation start</h3>
        <p>I started by running the annotator tool locally and seeing what is the best way to implement pagination in the backend. The annotator tool is comprised of a frontend that is made in ReactJS, a backend that is created with a Python framework named Flask. The data is stored in a totally different project called its-api, which acts as the api, duh, of the system. It is a REST API that follows the OpenAPI specification of building APIs. Even though I read the specification there was not a section that clearly explained pagination. I tried running its-api locally, but I couldn't, even though I followed the Readme file, which conatined the step-by-step guide to run the its-api project. I decided to leave it for tomorrow.</p>
        <h3>09.02.2023 | Issues with the implementation</h3>
        <p>Nothing worked while trying to run the api project. I got tired after a bit and I decided to start implementing a mock pagination for the local backend and the frontend. I did that so I could waste little time when the actual implementation starts (I knew that frontend tasks take me more time). I started by implementing it in the local backend. I created a function called pagination and added some parameters - page and per_page. The page paramenter serves as the number of the page the user is currently on and the per_page parameter refers to the amount of items that are shown, in this case we had 5, 10 ,15, 20, 25 and 30 items that could be shown per page. I also implemented some functionts that validated the user input. I implemented this in the backend and started working on the frontend. The way pagination worked in the frontend was weird, it was client side pagination made by past students. I decided to use their pagination and table components to use it in the problematic page. I tinkered the components and made them so they worked exactly as intended. This "mock" implementation took me until the end of the week to finalize.</p>
        <h3>13.02.2023 | Running its-api locally</h3>
        <p>During the daily standup meeting at 10:00 I asked for some assitance in running the its-api project locally. One of the project maintainers - Redmar, helped me with setting up the environment to run the project. I was missing the <b>git lfs</b> extension from my repository and it was needed to download the larger files. I was also missing an access token, neither of those things were mentione in the step-by-step guide. After receiving the token I tried running the project, but again I was unable to run it locally. At this point Redmar did not know what to do so I decided to let him be and try to run it myself, now that I had all the necessary things in place. My intuition told me that the most obvious reason for me not being able to run the project locally, was that I was using Docker Desktop, instead of the normal Docker engine. I had read that Docker Desktop caused some problems when running some projects because of some privilage issues. For the sake of time I was not interested in handling privilage issues in the Docker Desktop application, so I just deleted it and installed Docker engine. After doing that I went back to the handbook and I read about the process of installing docker engine locally and so I followed it and tried again. This time it worked and I managed to run the project. I was very happy about it and I could finally start applying pagination for the server.</p>
        <h3>14.02.2023 | Server implementation</h3>
        <p>After finally running the project locally I went into the its-api code and started the implementation of the pagination there. It made use of <a href="https://www.sqlalchemy.org/">SQLAlchemy</a> flask version, which already has everything needed to paginate the results that are in the database. I made use of the Pagination object, which already had all the details that I needed to return to the client. I changed the path to accept query URL parameters and extracted the page and per_page parameters out of the url link. After extracting those parameters I used them to query the data in the database and return the paginated object. I had a problem with this though, as I could not return a response different from 200 for some reason.</p>
        <h3>15.02.2023 | Success!</h3>
        <p>While I was looking into the issue with the return not working correctly I noticed that I had a file related to the openAPI specification called api-internal.yaml which contained a JSON object with the return value of the get_all function for organizations. I saw that the return value was an object. I changed it to a Dictionary and returned all the necessary details for the paginated object and all the details needed for the frontend to work correctly. After this file was changed I also added a description to all parameters that are received in the URL query string. This marked the feature as complete and now what was left was to write some unit tests and test the functionality.</p>
        <h3>16.02.2023 | Automated testing for pagination</h3>
        <p>It is my birthday! I ran the project locally again and this time I ran the automated pytest tests. I scrolled down to the organization tests and added a few more to them. I added a tests where you fetch a certain page in the api and one more tests that fetches a certain number of organizations in a certain page. This ensured that the pagination for organizations is well tested and functional. I pushed everything to the Issue I created in GitLab and I set Redmar as the reviewer</p>
        <h3>20.02.2023 | yarn to npm</h3>
        <p>This week started with a Daily meeting in which I explained how the new pagination for organizations worked. Turns out my implementation fixed another problem related to the end-to-end tests not working correctly. After pagination was implemented they started working again. After I communicated this with the team I moved on to another issue that the company wanted me to check out. This issues is in the annotator tool again and it is moving from yarn to npm (again). I started this by creating a new branch called <b>yarn-to-npm</b>. I started by taking a look at the <b>package.json</b> file, where all the commands and packages were stored. I change the commands to npm and removed yarn from it. Afterwards, I ran the command <b>npm install</b> to install all needed packages and removed the <b>yarn.lock</b> file. The install command ran successfully and I moved on to test the application. Everything built correctly, but I noticed an issue with the security threats that npm found. I ran the <b>npm audit</b> command and saw that there were a total of 60+ security threats in the application.</p>
        <h3>21.02.2023 | Package update galore</h3>
        <p>I started the day off with a lot of package updates. I managed to break the application a couple of times and I had to rewrite the <b>package.json</b> file multiple times. I ran another command that tested the application better - <b>npm audit --omit=dev</b> this command did not take into account the packages that were used only in the development environment. I saw that we still had an issue with 3 high level security threats that existed in the production environment. I updated a total of 8 packages to more current versions and tested if the security threats still existed, but thankfully everything was working correctly. I pushed to the repository and saw that I missed some yarn commands in the configurations file, which I promptly fixed. After this I pushed the initial stages of the CI/CD pipeline passed, which were testing and linting. But e2e tests failed, because of missing artifacts. I messaged Nikita, a member of the company that works in developing the annotator tool, and asked him if he knew about this issue. He informed me that he could speak after the sprint delivery is complete. At the end of every two weeks we hold a meeting, in which everyone shares what they completed during the 2 week sprint. I also participated in that meeting and after the meeting was done I held the meeting with Nikita.</p>
        <h3>23.02.2023 | Finishing touches</h3>
        <p>After waiting a bit for Nikita to fix the problem I encountered, I decided that it would be best if I take care of it. I took a look in the pipeline and saw that one of the lines there was incorrect <b>npm run cypress run</b> I tested this in my local environment and it did not run, so I tried with <b>npx</b>, which is made for this. I changed the command to <b>npm cypress run</b> and this successfully ran the e2e tests locally. I changed the pipeline to include this command and the tests passed! This marked the move from yarn to npm as complete. All I had to do now is update the documentation and change all instances of yarn to npm. I pushed my changes and tagged Nikita as my reviewer.</p>
        <h3>24.02.2023 | Handbook problems</h3>
        <p>Today Kees tagged me in a couple of issues that he found in the handbook. He had applied a new linter to the handbook files and there were around 40 linting errors that needed fixing. Since I was in my warmup month he wanted me to fix them. I started by cloning the handbook repository locally and running the new branch he had created with the linter. I ran the command that shows me all the incorrect files. I saw that there were multiple files with missing or unused links in them so I decided to go file by file. Some of the files were false positives, and there weren't any actual errors, more like warnings. I disabled the linting tool warnings, after making sure that they were all the same. I was still left with some files that had missing links, so I had to go file by file and I added the urls for reach link. I made sure that everything was set for the push. After I made sure to test locally I pushed the updated versions of all the fixed files, but I got an error associated with a cpp file, which did not exist in my local machine, but I guess it existed on the pipeline. This prompted me to read the linter's documentation again and I saw that there was a config that could be used to ignore certain file types, I used that and everything was set. I pushed my changes and assigned Kees as reviewer, he accepted them and the new version of the handbook was submitted.</p>
        <h3>27.02.2023 | Linting!</h3>
        <p>When the move from yarn to npm was completed I moved on to another task that I had from this month and this was applying some more backend linters to the project pipeline. I chose one additional linter called <b>pylint</b> which rates the code from a scale of 0 to 10 if it fits the linting standards. We were fortunate enough to hit a 10 out of 10 on the first run. After I added this linter I also updated the confugiation so that it did not include migration files. I added this to the pipeline and submitted the fix for the issue. I am still awaiting review on this.</p>
        <h3>28.02.2023 | yarn to npm again</h3>
        <p>Today the merge request I got for the yarn to npm update got accepted, and Nikita resolved all threads on it. We are waiting for Kees to accept it. Meanwhile I went and tried to research frontend linting. Which probed to be more of a challange than the backend linting, simply because there were more linters to be added. I saw three more linters that I could add looking at the <a href="https://gitlab.com/appsemble/appsemble">Appsemble</a> project and I might be able to add them after yarn to npm is accepted. I also messaged my teacher for the Project Plan and asked him to review it again. I hope that I can submit it this week.</p>
        <h3>01.03.2023 | Canvas and uni</h3>
        <p>It's March, which means that today we celebrate Baba Marta day in Bulgaria. This did not stop me from working though, the yarn to npm still was not accepted to be merged so I am waiting. In the meanwhile I looked in Canvas and created a plan for the documents that I planned on creating. I focused on keeping everything that I mentioned in the Project Plan and made sure to check the deadlines for the whole semester again</p>
        <h3>02.03.2023 | Documentation Day!</h3>
        <p>I made sure to get all documents I needed ready for the university like my Project Plan. I also worked on my blog, so I can ensure that everything is well documented. Moreover, I created a plan and wrote all the important deadlines for the documents, and planned ahead to make sure everything is on time when it comes to documentation.</p>
        <h3>06.03.2023 | Frontend linting</h3>
        <p>I researched some linters for the frontend and also looked into how <a href="https://gitlab.com/appsemble/appsemble">Appsemble</a>, the project made use of their frontend linters. I saw that they make use of five different linters for the frontend - <b>cspell</b> | <b>helm lint</b> | <b>prettier</b> | <b>stylelint</b> and <b>remark lint</b>. For the annotator tool I am leaing towards using <b>cspell</b> and <b>stylelint</b> as they fit the project more than the other linters. When I start adding them I will also speak to Kees and the other collgues to see if they think of additional linters for the frontend.</p>
        <h3>07.03.2023 | e2e test issues</h3>
        <p>When the yarn to npm was merged into master we had some issues with the e2e tests. The e2e test failed for some reason and I had to investigate why. I started by taking a look at the CI/CD pipeline and saw where cypress was called and looked into the command, everything seemed fine so I went to see the artifacts that were created by cypress. It automatically creates a video of the e2e tests that failed so I can see and debug. I saw that one of the tests that was failing could not updated the data, because a "dueDate" was not set for a certain assignment, so the form gave an error when updating that assignment. I went on to investigate further and found out that the database that is used when merging to master is defferent that the database used during a merge request, the merge to master database is the staging database and the data there was really stale from 2021, this meant that some of the values there did not fit with the current requirements. I communicated with Redmar about this, as he had access to the database, we updated the values and added dueDates to every assignment then restarted the pipeline and the e2e tests were working again, hopefully they won't break anymore!</p>
        <h3>08.03.2023 | cspell</h3>
        <p>I with implementing the cspell linter for the frontend. I first went over the cspell documentation and read how to use this linter. I created a configuration file that removed certain words from the linting requirements, I saw that there was a multitude of files that were using Dutch and this linter wrongly assumed that they were spelling errors, I started reseraching if there was a way to include dutch to the cspell and I am currently still checking this issue.</p>
        <h3>13.03.2023 | Backend linting</h3>
        <p>I was told by my mentor that I had to improve the backend linter that I previously added, as it did not lint correctly. This was because I used the <b>--disable all</b> flag and did not make use of pyenchant as my spelling library. I went into that issue again and instead of using the <b>--disable all</b> flag I decided to create e <b>.pylintrc</b> file that has all the configurations there, this meant that I could use little to no commands for the linter when calling it in the terminal. When I did that however I was faced with numerous linting errors and a code score grade of 4. I started by reading some of the linting warnings and saw that a large protion of them were false positives, especially the spelling. A lot of the technical words that were used were marked as spelling errors, which was not the case. I created a dictionary filled with the technical terms that were used accross the project and added the <b>spelling-private-dict-file</b> flag in the <b>.pylintrc</b> file. This improved the grading of the code by 2 points! Afterwards I went on to check the other errors and saw that some of them were related to the amount of parameters that some functions used. I increased some of the limits for the function parameters, as those functions could not work with less. Then I saw that the linting warnings that were left were the real ones and I started fixing the linting problems throughout the project. I fixed the migration files and the source code that contained a lot of typos and other inconsitencies located by the linter. When I completed this I added <b>- pipenv run pylint --rcfile=.pylintrc --enable spelling itsannotate/*.py</b> to the <b>.gitlab-ci.yml</b> file. After I added my changes and commited them the unit tests and the linter failed. The day was nearing it's end and I decided to progress with this task tomorrow.</p> 
        <h3>14.03.2023 | Versioning issues</h3>
        <p>Yesterday I did not manage to completely fix linting as I stumbled upon issues with the unit tests and the linting in the CI pipeline. First I wanted to see why the unit test failed, so I went to the CI Job page and investigated what went wrong. The error there pointed at an incompatible <b>SQLAlchemy</b> version, which prompted me to try and downgrade the <b>flask-sqlalchemy</b> package. After doing that I did not see any difference, I though about what could be the cause of it and after failing a lot of times and trying not to updated the version of the packages but only to add pylint and pyenchant I saw that there is no way to avoid not updating all the packages. This meant that I HAD to find the problematic package, but I did not really know where it was located as I already tried removing the flask-sqlalchemy package and it did not work. This prompted me to methodically delete packages and check the <b>Pipfile.lock</b> if what I did worked. After doing this a bunch of times I saw that the thing that was messing up the sqlalchemy was the <b>flask-migrations</b> package, it seemed like that package installed both sqlalchemy and flask-sqlalchemy. After locating the bad package I downgraded it and I decided to add sqlalchemy as a seperate package and not to be dependant on the version that flask-migrations provided. I specifed which version of sqlalchemy I wanted in the project, added my changes to a commit and I pushed. This fixed the unit test failure but it did not fix the linting problem. I reserached this issue and I saw that pyenchant was not found in the project, I asked my mentor what he thinks the solution to this was and he pointed me to an old issue that they had in the company and I saw that I was missing a <b>before-script</b> command in the pipeline, where pyenchant could be installed. I added the <b>apt-get update -qq && apt-get install -y python3-enchant</b> command to the pipeline and everything worked! My changes were merged into master afterwards.</p>
        <h3>15.03.2023 | Frontend Linting Finale!</h3>
        <p>After researching the frontend linters in <a href="https://gitlab.com/appsemble/appsemble">Appsemble</a> I added two linters - <a href="https://stylelint.io/"">stylelint</a> and <a href="https://cspell.org/"">cspell</a> as official frontend linters. I saw that most of the linters that I mentioned before were already in place except cspell and stylelint. For cspell that I added before I continued my work and made sure to apply the linting suggestions by the litner. I had to change 30 files and fix a lot of typos and even some file name typos. I had to change some of the code also to facilitate these new changes. Afterwards, with stylelint I had to do some version changing to ensure that pylint worked with the <a href="https://styled-components.com/">styled-components</a> that we were using everywhere in the frontend. I did this because styled components were not supported anymore with this newer versions of this linter, only the old ones. Kees and me had a discussion about the viabilty of using styled components after this.</p>
        <figure>
            <img src="images/linting_issues_example.png" class="m-img">
            <figcaption>Image 1 - Example of some of the linting issues.</figcaption>
        </figure>
        <p>After adding these two linters I made sure that the frontend is properly linter and this marks my initial tasks as completed.</p>
        <figure>
            <img src="images/issues_first_month.png" class="m-img">
            <figcaption>Image 2 - Marked all my issues as complete.</figcaption>
        </figure>
        <h3>16.03.2023 | Sprint delivery and planning</h3>
        <p>In the daily standup I was told that we are to have a meeting aftewards to discuss my progress and to plan the next sprint and see which issues I am going to tackle to improve the annotator tool. We started by going over what I did this sprint namely:</p>
        <ul>
            <li>yarn to npm</li>
            <li>backend linting</li>
            <li>frontend linting</li>
            <li>e2e test fix (again)</li>
        </ul>
        <p>After presenting what I did we went over the existing issues in the system, the release is coming up soon and we need to get the bugs sorted before the clients test out the system. The first bugs we are going to sort out are related to the saving of annotators. Afterwards, we will remove the Expert Annotators, and we will start with the planning and implementation of the annotator scoring system, which is the main goal of the internship, as this is how I plan to improve the AI model and the annotations. Instead of grouping the annotators into Experts and Juniors I suggested that we use a score based system and take into account the opinion of the annotator with the better score. This will be done behind the scenes to ensure that the annotators are not affected by their score, instead they will have to use their expert skills to identify incorrectly pronounced words. This system will ensure that what we are getting the least amount of mis interpreted words by the experts.</p>
        <figure>
            <img src="images/board_17_03.png">
            <figcaption>Image 3 - Sprint Planning board - 16.03.2023</figcaption>
        </figure>
        <h3>17.03.2023 | Bug Fixing</h3>
        <p>After the sprint planning I went right to work and started trying to reproduce some bugs. What I found is that a lot of the bugs that I was assigned to could not be reproduced. This did not surprise me much as the bugs that were assigned to me were very old up to 1 year, and they might have been fixed. I saw that in some of the descriptions of the issues the CI pipeline was mentioned. Multiple programmers mentioned that it did not load sometimes during the e2e testing. This lead me to believe that the e2e tests were problematic for a very long time. After I fixed the e2e tests a couple of weeks back I haven't seen the pipeline break again, so I suspect that when I applied server pagination I also fixed the e2e tests, as this caused heavy of load on the server and may have caused the CI pipeline to fail a couple of times. I put four bugs in the closed section, I will work on the other ones next week and see if I can actually find a bug that exists in the application from one of the issues.</p>
        <h3>20.03.2023 | Bug: Visiting an annotation from the URL crashes</h3>
        <p>Some of the bugs that were assigned to me did not exist anymore, but after I tried visiting the annotation URL by using the browser link I saw that it indeed crashed, and the screen remained white. I went on to investigate this bug further and saw that the recording object was not loaded when the URL was entered in the browser. After I tried clicking on the annotation, instead of using the URL it worked. This prompted me to believe that the recording object is being generated and passed down to the annotation page after the link is clicked. This is not optimal as it leads to bugs like this. For this but I decided to look through the code and see if the recording object can be easily fetched by me. I saw that the object was fetched via an API request to the backend. I saw that the colleague that created this made use of <a href="https://redux-saga.js.org/">redux sagas</a>. Redux is a state container and sagas are a way to manage side effect that happen in asynchronous calls. In this case redux makes use of <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/function*">Generator</a> functions that can be exited an re-enterd in a later point. They eliminate the problem with callbacks and avoid callback hell. I've not used these types of functions and moreover I saw that redux was applied in a lot of folders in the project. The redux that I have used before simply held all the states in one file and one object, but in this project it was way more complex. This is why I decided to save much needed time and I went for a different approach. Instead of fixing the crash by loading the recording object using the ID of the recording that is passed in the URL I decided to check if there is a recording object and if there isn't I redirect the client back to the Dashboard, where they came from. This is good for multiple reasons. The first one is that the clients that will use the application will very rarely use the URL in the first place, they would much rather click on the annotation. Moreover, this implementation saves time, minimizes the risk of bugs and does a good job in handling the problem in the most cost effective way. In the end I fixed this bug by redirecting the user to the Dashboard, if the recording object was not loaded.</p>
        <pre><code>
    try {
        const {
        location: {
            state: { recording },
        },
        } = props;

        wavesurfer.current.load(
        `${process.env.ITSL_APP_API_URL}/recording/${recording}/audio`,
        );
    } catch (error) {
        window.location.href = '/dashboard';
    }
        </code></pre>
        <h3>21.03.2023 | Bug: Broken SMTP server</h3>
        <p>One of the other issues the company had was that the SMTP requests always returned an OK code, even when the email was not sent. This is not correct behavior as the application lies to the user that the email has been sent, while in reality it has not. I went to check the code for the <b>send_email()</b> function and saw that there was a <b>try catch</b> block associated with it. Moreover, I saw that in the <b>catch</b> block when an error occured it simply logged in the console that the error existed and that was it. I investigated further and saw that there was not really an email client that was used to sent mail. The only configuration I saw in the <b>config.py</b> file was that a production config, and no mail server in development. This prompted me to think that the email was only used in production and for the actual application there was no email that was being sent or used. This was communicated with Kees the next day.</p>
        <h3>22.03.2023 | Adding an SMTP server</h3>
        <p>In the morning standup meeting I told Kees what my findings were and we discussed a solution for this. We decided to use <a href="https://github.com/mailhog/MailHog">Mailhog</a> an email server that is used by developers. This is useful as it does not send email to any address, but it acts like an SMTP server and does not bother anyone while in development. Since I did not know about email servers in detial, as I have set up only two in my three years as I developer I decided to refresh my knowledged and conducted some research on this topic and saw a few video tutorials about how SMTP server were used. I decided to use <b>Docker</b> and I added Mailhog as a container in the <b>docker-compose.yml</b> file.</p>
        <pre><code>
    mailhog:
    image: mailhog/mailhog
    command: sh -c adduser -D -u 1000 mailhog
    ports:
        - "1025:1025"
        - "8024:8024"
    networks:
        main:
        aliases:
            - mailhog.itsl        
        </code></pre>
        <p>After adding this to the <b>docker-compose.yml</b> file I tested it out and it worked!! Then I ran the <b>pipenv run test</b> command in my project's virtual environment and there was something wrong (of course!) and I had to see what. I was getting an OS exception when running the unit tests and the word <b>port</b> was mentioned, this immediately prompted me to check if something is blocking the port where my mail server was running on. I saw that in the tests they make use of ANOTHER mail server that is especially designed to run in unit tests, but surprise surprise it ran exactly on port <b>8025</b>, where my email server was running on. I had an easy solution to this though, I went to the <b>docker-compose.yml</b> file and specified that I wanted Mailhog to run on port <b>8024</b>. After I ran the tests again there was not problems. I added my changes and pushed to the remote repo in the new branch that I created. I received a linting error, and at this point we were using three seperate linters for the backend so I just decided to make my life easy and I created an shell file that ran all the linters in one go, so I don't repeat the same mistake or run all linters one by one. This is how my shell file looked like.</p>
        <pre><code>
    #!/bin/bash
    pipenv run black .
    pipenv run flake8 .
    pipenv run pylint --rcfile=.pylintrc --enable spelling itsannotate/*.py
        </code></pre>
        <p>Hopefully, this will prevent linting problems. But oh no the CI pipeline failed, and this time the unit tests the problem. I went to see what was wrong and saw that I forgot to add the image to the <b>.gitlab-ci.yml</b> file. After adding mailhog to the CI services section I ran the tests again and they failed... again. But this time it was a different error. Well kind of different, it was the port issue again. I saw that the other mail server used in the unit tests (<a href="https://pypi.org/project/smtpdfix/">smtpdfix</a>) still conflicted with mailhog. This time the problem was not so easy to solve as the image that is pulled from the dockerhub is located in the CI pipeline and not the compose file. This is why I decided to ask my mentor for directions the next day...</p>
        <h3>23.03.2023 | Finishing SMTP touches</h3>
        <p>In the daily standup I shared my troubles with my mentor - Kees, and he gave me an idea to use the remote URL to out own Mailhog server hosted in the office. I obliged and I chagned the <b>SMTP_SERVER</b> variable in the <b>config.py</b> file. To my surprise it did not work. I pondered a little and saw that the mail was not being sent at all. At first I thought it was a permission issue, and I might have needed a password for the server, but after checking for a bit I saw that I've forgotten to add a username... Anyways, after adding the username I successfully sent the email and I pushed the changes. This time everything worked fine and the tests passed. Marking this fix as done. Since the day was not really done I decided to move to another issue - The removal of the Expert annotator. This issue is part of my assignment goal - to try and improve the capabilites of the AI model by improving the input of the annotators. I belive that removing the expert annotator role and focusing on a score would be a much more beneficial system to train the AI with. Anyways, I started as always by researching and seeing how the expert annotator is being used in the application. That's what I did basically for the whole day.</p>
        <h3>24.03.2023 | Removing the Expert Annotator role</h3>
        <p>After dealing with all of the bugs that were assigned to me the time has come to remove the expert annotator role. We had communicated earlier and even during the first interview with my mentor - Kees that in order for us to improve the AI model we would need to remove the differential factors between the annotators and instead of seperating them into <b>Expert</b> and <b>Normal</b> annotators we could use a scoring system. This scoring system would be based on different factors that are related to whether or not the annotations that the certain annotator has made have been deemed incorrect or correct. These factors are not yet set. The first step to this is removing the expert annotator role from the project and giving the normal annotator all functionalities of the tool. I started this by taking a look where the roles are defined and in this case it was in the <b>user.py</b> class.</p>
        <pre><code>
    class UserRole(Enum):
    ADMIN = "ADMIN"
    ANNOTATOR = "ANNOTATOR"
    EXPERT_ANNOTATOR = "EXPERT_ANNOTATOR"
    EXTERNAL_ANNOTATOR = "EXTERNAL_ANNOTATOR"

    @staticmethod
    def list():
        return list(map(lambda l: l.value, UserRole))

    def __str__(self):
        return self.name
        </code></pre>
        <p>After taking a look at this code I can see that the UserRole contains the Expert Annotator role, so this means that if I remove this role and then change all additions of this role to just an annotator role, the backend should be all set. I went on to remove <b>EXPERT_ANNOTATOR = "EXPERT_ANNOTATOR"</b> from the class <b>UserRole</b>. Afterwards I went in the <b>api/</b> folder and removed the <b>required_role(UserRole.ADMIN, UserRole.EXPERT_ANNOTATOR, UserRole.ANNOTATOR)</b> authorization endpoint to feature only the base annotator role. After this was done I went to the <b>/migrations</b> folder and made sure to add base annotators instead of expert annotators, when the database is being migrated at the start of the application. When I did this I moved to the <b>/test</b> folder and changed some of the tests that were associated with the expert role to instead use the base annotator role. After this was done the backend was free of the expert role. I moved to the frontend and started getting rid of it there. First, I used <b>grep -rnw '/frontend' -e 'expert'</b> to locate where the role was used. After locating where it was used I removed every occurance of it. After being done with the frontend I ran all automated tests, just to make sure that everything ran correctly and the application was still working. I pushed everything and created a merge request.</p>
        <h3>27.03.2023 | Adding more specific exceptions</h3>
        <p>The last issue of this Sprint was the addition of more specific exceptions for the backend. Some of the <b>try catch</b> statements in the backend are catching general errors, this works alright when it comes to avoiding server errors being sent to the client, but for readability purposes it is better to use more specific exception handling. Below is a representation what kind of try catch blocks we had in the application.</p>
        <pre><code>
    try:
        smtp.send_message(msg)
    except Exception as ex:  # noqa
        logging.warning(f"SMTP Exception: {ex}")
        raise ex
        </code></pre>
        <p>Pay attention to the <b># noga</b> comment. This means that the linter tools that we have in the backend skip this line when they see this comment. This is because a lot of the existing linter tools do not allow general exceptions, since they impact code readability. These lines won't be needed after I am done with the code. Below is how I changed this particular line of code.</p>
        <pre><code>
    try:
        smtp.send_message(msg)
    except smtplib.SMTPException as ex:
        logging.warning(f"SMTP Exception: {ex}")
        raise ex
        </code></pre>
        <h3>28.03.2023 | End of Sprint 4 - Sprint Delivery and Planning</h3>
        <p>I've reached the end of this sprint! This means that today we are going to have a sprint retrospective and discuss what happened during the last two weeks. We are going to listen to each other and maybe see what each of us has accomplished this sprint. After this is done we can move on to Sprint Planning and we will plan issues for the next sprint. Currently I have completed all my issues this sprint and I will need to see if there are any comments on them, if there are I will first need to make sure that the comments are resolved and that everything is taken care of before going into the next sprint. I will go over the issues I have completed in the list below with their issue numbers in GitLab.</p>
        <ul>
            <li>#150 | Introduction of the expert annotator</li>
            <li>#210 | Exception handling should be more specific</li>
            <li>#256 | Return an error status when sending an email fails</li>
            <li>#299 | Visiting the annotate page from the url crashes</li>
            <li>#307 | Changing not saved annotations</li>
            <li>#208 | Checkbox does not click in Cypress</li>
            <li>#267 | Sometimes recordings don't load on the assignment page</li>
            <li>#310 | Starting a region in an other region</li>
            <li>#309 | Regions without labels</li>
        </ul>
        <h3>29.03.2023 | Frontend linting finishing touches</h3>
        <p>My mentor took a lot at the code for frontend linting and saw that there were an issue that I forgot to address. I saw that there was a file with a incorrect path set in the changes that I pushed whiled applying the frontend linting. I saw that all I needed to change was one line directing to a certain component. I applied this changed and pushed the code to master. But disaster struck, I had forgotten to merge master into my branch before pushing, this meant that the new linting that I applied caused the new changes that came AFTER it to crash, I saw this while checking the CI pipeline, but the change was done already. So now I had to create another Issue to address this mistake and fix the linting issues that occured in master.</p>
        <h3>30.03.2023 | Mailing issues</h3>
        <p>After the meeting we had on Tuesday Kees shared that he would prefer the email to no be connected to the remote mailing server, instead he wanted it to be a docker image. I explained that since we are already using a mock email server in the unit tests we cannot use the <b>mailhog</b> image form the CI pipeline, as the docker image in the pipeline uses the same port mapping as the <b>smtpdfix</b> Python package we are using for the unit tests. He showed me one page that let me configure the variables of <b>Mailhog</b>, but I was not certain how to configure the variables of an image insite a GitLab CI pipeline, he explained that you can set the environmental variables from the CI, if you supply the <b>variables</b> extension command in the <b>.gitlab-ci.yml</b> file. I took his word for it and tried this, I supplied the needed variables and tested it. I saw that this did not work as you cannot map the ports and open them so they can be used by the unit tests, without having access to the <b>port</b> comman Docker offers. I tried other tricks to make this work, but whatever I did it was futile. I could not make it work whatever I tried, so I just decided that I will postpone this for Monday and work on another task more closely associated with the goal of my internship.</p>
        <h3>31.03.2023 | More specific annotations round two</h3>
        <p>I tackled this issue before, but Kees told me that he would also like to see tests associated with the new error handling that I implemented to avoid general exceptions. I saw from the merge requests which lines were not tested properly and I went to update them. I started with the annotation exception handling and I created two tests for it. After this I moved to the exception catch for the database and I saw that some of the errors that I used were incorrectly added, and I had to be more specific about the error class that is being used <b>DataError</b> was located inside the <b>sqlalchemy.exc</b> class and I had to make sure I imported that correctly, so this is where the unit tests I wrote actually benefited me. I moved on two create a couple more tests and I pushed my commit, hopefully it will go well this time and it will accepted.</p>
        <h3>01.04.2023 | </h3>
        <p>SOON TO BE ADDED</p>
    </div>
</body>
</html>
